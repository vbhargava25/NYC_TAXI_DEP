###End-to-End Data Pipeline for NYC Taxi Trip Analysis Using Azure Databricks

![img alt](https://github.com/vbhargava25/NYC_TAXI_DEP/blob/main/cf.png?raw=true)

ğŸ› ï¸ Step-by-Step Pipeline
1. ğŸ“‚ Data Understanding
Explored the schema, types, and business value of the NYC Taxi dataset.

2. ğŸ”§ Azure Databricks Setup
Created a Databricks Workspace and launched compute clusters for ETL workloads.

3. ğŸ” Secure Data Access
Configured Azure Active Directory Service Principal for secure authentication.

Granted appropriate read/write permissions to Azure Data Lake Storage (ADLS).

4. ğŸ”— Lakehouse Integration
Used credential passthrough to authenticate Databricks access to ADLS.

Verified data visibility and structure within the data lake.

5. ğŸ¥‰ Bronze Layer â€“ Raw Ingestion
Ingested raw CSV/Parquet files using PySpark.

Stored as-is in the Bronze layer for traceability.

6. ğŸ¥ˆ Silver Layer â€“ Cleaning & Transformation
Applied schema enforcement and column renaming.

Removed invalid records (e.g., zero fare, zero distance).

Extracted useful time-based features (year, month).

Stored transformed data as Parquet files.

7. ğŸ¥‡ Gold Layer â€“ Aggregations & Delta Tables
Created Delta Tables for analytics (e.g., trips per day, avg fare by zone).

Enabled time-travel and efficient querying.

8. âš™ï¸ Automation
Built and scheduled Databricks Jobs to update the pipeline daily.

9. ğŸ“ˆ Analytics & Reporting
Queried Delta Tables using SQL within Databricks.

Connected to Power BI to build interactive dashboards for insights.

âœ… What We Achieved
ğŸš€ Implemented a modular, scalable data lakehouse pipeline.

ğŸ” Ensured automated, secure, and versioned data workflows.

ğŸ“‰ Delivered actionable business insights through dashboards

ink used https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page
